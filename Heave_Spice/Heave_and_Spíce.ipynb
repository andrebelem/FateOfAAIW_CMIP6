{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook Description and Execution Requirements\n",
        "\n",
        "This notebook aims to analyze and visualize the **Heave** and **Spice** components of ocean salinity and temperature, derived from CMIP6 climate model data. The analysis focuses on zonal-mean anomalies (future vs. historical) within a specific depth layer.\n",
        "\n",
        "### What the Notebook Does:\n",
        "\n",
        "1. **CMIP6 Data**: CMIP6 Data Retrieval with gdown: Transfers preprocessed NetCDF (.nc) files—containing salinity and temperature data from multiple CMIP6 models for historical and projection scenarios (ssp370, ssp585)—into the notebook environment using gdown. These files were previously prepared and are essential for the analysis.\n",
        "3. **Definition of Auxiliary Functions**: Includes functions for robust interpolation, calculation of the Heave and Spice components (using the `neutralocean` library), alignment of depth coordinates between datasets, and determination of “nice” limits for contour plots.\n",
        "4. **Data Processing by Model and Scenario**:\n",
        "    * For each model and scenario, loads the historical and projection datasets.\n",
        "    * Computes the *spice* and *heave* fields for salinity and temperature.\n",
        "    * Computes the zonal mean of these fields and selects data within the 0–2000 m depth range.\n",
        "    * Saves these preprocessed zonal-mean data into individual NetCDF files (`preprocessed_data/zonal_mean_MODEL_SCENARIO.nc`).\n",
        "    * Records the maximum absolute values of each component (Heave/Spice for salinity/temperature) in a JSON file (`raw_contour_limits.json`).\n",
        "5. **Normalization of Contour Limits**: Adjusts the color and contour limits of the plots to ensure consistency and symmetry across all models and scenarios, enabling fair visual comparison. The finalized limits are saved in `final_plot_limits.json`.\n",
        "6. **Ensemble Mean Calculation**: Computes the mean across all processed models for each scenario, creating an ensemble mean for the Heave and Spice components. These ensemble data are saved in `preprocessed_data/ensemble_zonal_mean_SCENARIO.nc`.\n",
        "7. **Generation of Final Plots**: Produces detailed visualizations for each individual model and for the ensemble mean. The plots display Heave and Spice components of temperature and salinity. Additionally, the ensemble mean includes **stippling** to indicate areas where anomalies are statistically significant (p < 0.05) based on a t-test across ensemble members.\n",
        "\n",
        "### Required Files and Folders for Running on Another Machine:\n",
        "\n",
        "To reproduce this notebook on another machine, you will need the following:\n",
        "\n",
        "* **NetCDF Data Files (`.nc`)**: The raw CMIP6 model data files downloaded via `gdown`. These are essential inputs.  \n",
        "  The notebook downloads them directly from Google Drive URLs, so the machine must have internet access, or you may manually download them and place them in the same folder as the notebook (`/content/`).  \n",
        "  The file names follow the pattern:  \n",
        "  `CMIP.MODEL.SCENARIO.Omon.gn.nc` and `ScenarioMIP.MODEL.SCENARIO.Omon.gn.nc`.\n",
        "\n",
        "* **Folder Structure (Generated by the Notebook)**:\n",
        "    * `preprocessed_data/`: Automatically created by the notebook to store intermediate results (zonal means for each model and the ensemble).\n",
        "\n",
        "* **JSON Files (Generated by the Notebook)**:\n",
        "    * `raw_contour_limits.json`: Generated after computing the raw contour limits.\n",
        "    * `final_plot_limits.json`: Generated after normalizing the contour limits.\n",
        "\n",
        "* **Output Images (Generated by the Notebook)**:\n",
        "    * `ssp370_temperature_plots_with_ensemble_stippling.jpeg`\n",
        "    * `ssp370_salinity_plots_with_ensemble_stippling.jpeg`\n",
        "    * `ssp585_temperature_plots_with_ensemble_stippling.jpeg`\n",
        "    * `ssp585_salinity_plots_with_ensemble_stippling.jpeg`\n",
        "\n",
        "  These JPEG files contain the final plots and will be saved in the notebook’s root directory.\n",
        "\n",
        "**In summary**: The main external requirement is the CMIP6 `.nc` data files. The notebook is designed to download them, and all other folders and JSON files are generated automatically during execution. The machine must have internet access for data download and installation of required libraries."
      ],
      "metadata": {
        "id": "Ik0j1bzoS0_U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fOnAMCjpD_P",
        "outputId": "4addb966-0232-4778-e655-33102a51b105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cftime\n",
            "  Downloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.12/dist-packages (from cftime) (2.0.2)\n",
            "Downloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cftime\n",
            "Successfully installed cftime-1.6.5\n",
            "Collecting xarray==2024.6.0\n",
            "  Downloading xarray-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from xarray==2024.6.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.12/dist-packages (from xarray==2024.6.0) (25.0)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.12/dist-packages (from xarray==2024.6.0) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->xarray==2024.6.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->xarray==2024.6.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->xarray==2024.6.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->xarray==2024.6.0) (1.17.0)\n",
            "Downloading xarray-2024.6.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xarray\n",
            "  Attempting uninstall: xarray\n",
            "    Found existing installation: xarray 2025.10.1\n",
            "    Uninstalling xarray-2025.10.1:\n",
            "      Successfully uninstalled xarray-2025.10.1\n",
            "Successfully installed xarray-2024.6.0\n",
            "Collecting gsw\n",
            "  Downloading gsw-3.6.20-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from gsw) (2.0.2)\n",
            "Downloading gsw-3.6.20-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gsw\n",
            "Successfully installed gsw-3.6.20\n",
            "Collecting netcdf4\n",
            "  Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.12/dist-packages (from netcdf4) (1.6.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from netcdf4) (2025.10.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from netcdf4) (2.0.2)\n",
            "Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: netcdf4\n",
            "Successfully installed netcdf4-1.7.3\n",
            "Collecting neutralocean==2.1.3\n",
            "  Downloading neutralocean-2.1.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from neutralocean==2.1.3) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from neutralocean==2.1.3) (2.0.2)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.12/dist-packages (from neutralocean==2.1.3) (2024.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from neutralocean==2.1.3) (1.16.3)\n",
            "Requirement already satisfied: gsw in /usr/local/lib/python3.12/dist-packages (from neutralocean==2.1.3) (3.6.20)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.12/dist-packages (from neutralocean==2.1.3) (1.8.2)\n",
            "Collecting xgcm (from neutralocean==2.1.3)\n",
            "  Downloading xgcm-0.9.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: netcdf4 in /usr/local/lib/python3.12/dist-packages (from neutralocean==2.1.3) (1.7.3)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.12/dist-packages (from netcdf4->neutralocean==2.1.3) (1.6.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from netcdf4->neutralocean==2.1.3) (2025.10.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->neutralocean==2.1.3) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch->neutralocean==2.1.3) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from pooch->neutralocean==2.1.3) (25.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch->neutralocean==2.1.3) (2.32.4)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.12/dist-packages (from xarray->neutralocean==2.1.3) (2.2.2)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.12/dist-packages (from xgcm->neutralocean==2.1.3) (2025.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from xgcm->neutralocean==2.1.3) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->xarray->neutralocean==2.1.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->xarray->neutralocean==2.1.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->xarray->neutralocean==2.1.3) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch->neutralocean==2.1.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch->neutralocean==2.1.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch->neutralocean==2.1.3) (2.5.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.12/dist-packages (from dask->xgcm->neutralocean==2.1.3) (8.3.0)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dask->xgcm->neutralocean==2.1.3) (3.1.2)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.12/dist-packages (from dask->xgcm->neutralocean==2.1.3) (2025.3.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from dask->xgcm->neutralocean==2.1.3) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from dask->xgcm->neutralocean==2.1.3) (6.0.3)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from dask->xgcm->neutralocean==2.1.3) (0.12.1)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.12/dist-packages (from partd>=1.4.0->dask->xgcm->neutralocean==2.1.3) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->xarray->neutralocean==2.1.3) (1.17.0)\n",
            "Downloading neutralocean-2.1.3-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.2/110.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgcm-0.9.0-py3-none-any.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xgcm, neutralocean\n",
            "Successfully installed neutralocean-2.1.3 xgcm-0.9.0\n"
          ]
        }
      ],
      "source": [
        "#Install libraries\n",
        "!pip install cftime\n",
        "!pip install xarray==2024.6.0\n",
        "!pip install gsw\n",
        "!pip install netcdf4\n",
        "!pip install neutralocean==2.1.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d1edac4"
      },
      "source": [
        "# @title Essential Imports\n",
        "import xarray as xr\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from matplotlib import cm\n",
        "from matplotlib.colors import ListedColormap\n",
        "from scipy import stats\n",
        "from neutralocean.traj import ntp_bottle_to_cast\n",
        "import gsw\n",
        "import matplotlib.ticker as mticker # Import MaxNLocator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloads my analysis file directly from Drive\n",
        "!gdown 1Q9rb8Hx9HOt1T962PtkrntK27t4N88FC #Downloads the file.\n",
        "!gdown 106h7gu0WGxVFdFWnapQU4maskdPbKT1n\n",
        "\n",
        "#Downloads my analysis file directly from Drive\n",
        "!gdown 1Q9rb8Hx9HOt1T962PtkrntK27t4N88FC #Downloads the file.\n",
        "!gdown 1oY0-iWcZ-9V0_uAMKKoHLW_P5PSIWVzj\n",
        "\n",
        "#Downloads my analysis file directly from Drive\n",
        "!gdown 13jhtN_mJ67yWA_hfIBa46sZWe1gdgYqj #Downloads the file.\n",
        "!gdown 1jL-gPxHb29z0ZLPHGX37t0kPUNkISv5C\n",
        "\n",
        "#Downloads my analysis file directly from Drive\n",
        "!gdown 13jhtN_mJ67yWA_hfIBa46sZWe1gdgYqj #Downloads the file.\n",
        "!gdown 19_1YRA-9xkdXA5WSCWyIohQaxWcOM3Oy\n",
        "\n",
        "#Downloads my analysis file directly from Drive\n",
        "!gdown 1bY2m7UlSL-VsbpbuyTM0KWTA9dxCYJ1D #Downloads the file.\n",
        "!gdown 1RFoWsRUHLVj2jnVMVXL3G4xI2cz57RHh\n",
        "\n",
        "#Downloads my analysis file directly from Drive\n",
        "!gdown 1bY2m7UlSL-VsbpbuyTM0KWTA9dxCYJ1D #Downloads the file.\n",
        "!gdown 1c6xKdMtM4HZZEV8OQzagYvFa7pMYx2aj\n",
        "\n",
        "#Downloads my analysis file directly from Drive\n",
        "!gdown 1GFnnZXe4wjkWSj78Iwf1Mbke-nb-nkKI #Downloads the file.\n",
        "!gdown 15syf8Oa0tXoM3PfWgQmU0VSFBFlJd6Xd\n",
        "\n",
        "#Downloads my analysis file directly from Drive\n",
        "!gdown 1GFnnZXe4wjkWSj78Iwf1Mbke-nb-nkKI #Downloads the file.\n",
        "!gdown 1UeCijOcs2mrSH-CsB7MYhCvEEbXgnlJj\n",
        "\n",
        "#Downloads my analysis file directly from Drive\n",
        "!gdown 1ncZLX0-218XRlWgAGyeIyj5Sut9QlwEL #Downloads the file.\n",
        "!gdown 11lFfQZlOB5vpRZEzwRJxGF3swvT9fndo\n",
        "\n",
        "#Downloads my analysis file directly from Drive\n",
        "!gdown 1ncZLX0-218XRlWgAGyeIyj5Sut9QlwEL #Downloads the file.\n",
        "!gdown 1xUNAkCFRUkuApdTQkXPAp42vLqaPWnHr\n",
        "\n",
        "#Downloads my analysis file directly from Drive\n",
        "!gdown 1bxaVTcR5veuUyZLhjTFC3fzH1a0Bsen7 #Downloads the file.\n",
        "!gdown 1lXituflJ0ft93lhxVoJxGJHZQeWD6vso\n",
        "\n",
        "#Downloads my analysis file directly from Drive\n",
        "!gdown 1bxaVTcR5veuUyZLhjTFC3fzH1a0Bsen7 #Downloads the file.\n",
        "!gdown 1AZ3FrHTzdhMwoa8gBUKDLtHStoyJQNHu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZpaig9KpKM1",
        "outputId": "4338b797-6023-4fef-d6fd-ce9a5f248ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Q9rb8Hx9HOt1T962PtkrntK27t4N88FC\n",
            "To: /content/CMIP.CNRM-CERFACS.CNRM-ESM2-1.historical.Omon.gn.nc\n",
            "100% 4.87M/4.87M [00:00<00:00, 148MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=106h7gu0WGxVFdFWnapQU4maskdPbKT1n\n",
            "To: /content/ScenarioMIP.CNRM-CERFACS.CNRM-ESM2-1.ssp585.Omon.gn (2).nc\n",
            "100% 4.87M/4.87M [00:00<00:00, 45.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Q9rb8Hx9HOt1T962PtkrntK27t4N88FC\n",
            "To: /content/CMIP.CNRM-CERFACS.CNRM-ESM2-1.historical.Omon.gn.nc\n",
            "100% 4.87M/4.87M [00:00<00:00, 119MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oY0-iWcZ-9V0_uAMKKoHLW_P5PSIWVzj\n",
            "To: /content/ScenarioMIP.CNRM-CERFACS.CNRM-ESM2-1.ssp370.Omon.gn.nc\n",
            "100% 4.87M/4.87M [00:00<00:00, 181MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13jhtN_mJ67yWA_hfIBa46sZWe1gdgYqj\n",
            "To: /content/CMIP.CAMS.CAMS-CSM1-0.historical.Omon.gn.nc\n",
            "100% 3.25M/3.25M [00:00<00:00, 173MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jL-gPxHb29z0ZLPHGX37t0kPUNkISv5C\n",
            "To: /content/ScenarioMIP.CAMS.CAMS-CSM1-0.ssp370.Omon.gn.nc\n",
            "100% 3.25M/3.25M [00:00<00:00, 39.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13jhtN_mJ67yWA_hfIBa46sZWe1gdgYqj\n",
            "To: /content/CMIP.CAMS.CAMS-CSM1-0.historical.Omon.gn.nc\n",
            "100% 3.25M/3.25M [00:00<00:00, 166MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19_1YRA-9xkdXA5WSCWyIohQaxWcOM3Oy\n",
            "To: /content/ScenarioMIP.CAMS.CAMS-CSM1-0.ssp585.Omon.gn.nc\n",
            "100% 3.25M/3.25M [00:00<00:00, 183MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bY2m7UlSL-VsbpbuyTM0KWTA9dxCYJ1D\n",
            "To: /content/CMIP.NCAR.CESM2.historical.Omon.gn.nc\n",
            "100% 3.90M/3.90M [00:00<00:00, 213MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RFoWsRUHLVj2jnVMVXL3G4xI2cz57RHh\n",
            "To: /content/ScenarioMIP.NCAR.CESM2.ssp370.Omon.gn.nc\n",
            "100% 3.90M/3.90M [00:00<00:00, 32.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bY2m7UlSL-VsbpbuyTM0KWTA9dxCYJ1D\n",
            "To: /content/CMIP.NCAR.CESM2.historical.Omon.gn.nc\n",
            "100% 3.90M/3.90M [00:00<00:00, 222MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1c6xKdMtM4HZZEV8OQzagYvFa7pMYx2aj\n",
            "To: /content/ScenarioMIP.NCAR.CESM2.ssp585.Omon.gn.nc\n",
            "100% 3.90M/3.90M [00:00<00:00, 14.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GFnnZXe4wjkWSj78Iwf1Mbke-nb-nkKI\n",
            "To: /content/CMIP.NOAA-GFDL.GFDL-ESM4.historical.Omon.gn.nc\n",
            "100% 2.28M/2.28M [00:00<00:00, 151MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15syf8Oa0tXoM3PfWgQmU0VSFBFlJd6Xd\n",
            "To: /content/ScenarioMIP.NOAA-GFDL.GFDL-ESM4.ssp370.Omon.gn.nc\n",
            "100% 2.28M/2.28M [00:00<00:00, 21.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GFnnZXe4wjkWSj78Iwf1Mbke-nb-nkKI\n",
            "To: /content/CMIP.NOAA-GFDL.GFDL-ESM4.historical.Omon.gn.nc\n",
            "100% 2.28M/2.28M [00:00<00:00, 193MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UeCijOcs2mrSH-CsB7MYhCvEEbXgnlJj\n",
            "To: /content/ScenarioMIP.NOAA-GFDL.GFDL-ESM4.ssp585.Omon.gn.nc\n",
            "100% 2.28M/2.28M [00:00<00:00, 16.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ncZLX0-218XRlWgAGyeIyj5Sut9QlwEL\n",
            "To: /content/CMIP.IPSL.IPSL-CM6A-LR.historical.Omon.gn.nc\n",
            "100% 4.87M/4.87M [00:00<00:00, 17.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11lFfQZlOB5vpRZEzwRJxGF3swvT9fndo\n",
            "To: /content/ScenarioMIP.IPSL.IPSL-CM6A-LR.ssp370.Omon.gn.nc\n",
            "100% 4.87M/4.87M [00:00<00:00, 93.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ncZLX0-218XRlWgAGyeIyj5Sut9QlwEL\n",
            "To: /content/CMIP.IPSL.IPSL-CM6A-LR.historical.Omon.gn.nc\n",
            "100% 4.87M/4.87M [00:00<00:00, 139MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xUNAkCFRUkuApdTQkXPAp42vLqaPWnHr\n",
            "To: /content/ScenarioMIP.IPSL.IPSL-CM6A-LR.ssp585.Omon.gn.nc\n",
            "100% 4.87M/4.87M [00:00<00:00, 76.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bxaVTcR5veuUyZLhjTFC3fzH1a0Bsen7\n",
            "To: /content/CMIP.MIROC.MIROC6.historical.Omon.gn.nc\n",
            "100% 4.10M/4.10M [00:00<00:00, 18.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lXituflJ0ft93lhxVoJxGJHZQeWD6vso\n",
            "To: /content/ScenarioMIP.MIROC.MIROC6.ssp370.Omon.gn.nc\n",
            "100% 4.10M/4.10M [00:00<00:00, 171MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bxaVTcR5veuUyZLhjTFC3fzH1a0Bsen7\n",
            "To: /content/CMIP.MIROC.MIROC6.historical.Omon.gn.nc\n",
            "100% 4.10M/4.10M [00:00<00:00, 18.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AZ3FrHTzdhMwoa8gBUKDLtHStoyJQNHu\n",
            "To: /content/ScenarioMIP.MIROC.MIROC6.ssp585.Omon.gn.nc\n",
            "100% 4.10M/4.10M [00:00<00:00, 263MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "093a8e62"
      },
      "source": [
        "# @title Helper Functions\n",
        "def _interp(x, y, xnew):\n",
        "    \"\"\"Robust 1D linear interpolation (ignores NaN and sorts x).\"\"\"\n",
        "    m = np.isfinite(x) & np.isfinite(y)\n",
        "    if m.sum() < 2:\n",
        "        return np.full_like(xnew, np.nan, dtype=float)\n",
        "    xx, yy = x[m], y[m]\n",
        "    if np.any(np.diff(xx) <= 0):\n",
        "        o = np.argsort(xx)\n",
        "        xx, yy = xx[o], yy[o]\n",
        "    return np.interp(xnew, xx, yy, left=np.nan, right=np.nan)\n",
        "\n",
        "def _heave_spice_sa(sa_h, th_h, p_h, sa_f, th_f, p_f):\n",
        "    \"\"\"\n",
        "    1D column -> (spice, heave) for SALINITY on p_f levels (full grid).\n",
        "    Inputs and outputs are 1D along 'lev'. Returns arrays of SAME size as p_f.\n",
        "    \"\"\"\n",
        "    p_f_full = p_f.copy()\n",
        "\n",
        "    ok_h = np.isfinite(sa_h) & np.isfinite(th_h) & np.isfinite(p_h)\n",
        "    ok_f = np.isfinite(sa_f) & np.isfinite(th_f) & np.isfinite(p_f)\n",
        "    if ok_f.sum() < 2 or ok_h.sum() < 2: # Corrected condition order\n",
        "        n = p_f_full.size\n",
        "        return np.full(n, np.nan), np.full(n, np.nan)\n",
        "\n",
        "\n",
        "    sa_h_v, th_h_v, p_h_v = sa_h[ok_h], th_h[ok_h], p_h[ok_h]\n",
        "    sa_f_v, th_f_v, p_f_v = sa_f[ok_f], th_f[ok_f], p_f[ok_f]\n",
        "\n",
        "\n",
        "    # neutral bottles (historical -> future)\n",
        "    s1_list, p1_list = [], []\n",
        "    for s0, t0, p0 in zip(sa_h_v, th_h_v, p_h_v):\n",
        "        s_ntp, t_ntp, p_ntp = ntp_bottle_to_cast(s0, t0, p0, sa_f_v, th_f_v, p_f_v)\n",
        "        s1_list.append(s_ntp); p1_list.append(p_ntp)\n",
        "    s1 = np.asarray(s1_list); p1 = np.asarray(p1_list)\n",
        "\n",
        "    # spice at historical depths -> mapped to future levels (p_f_full)\n",
        "    spice_hist = s1 - sa_h_v\n",
        "    spice_pf   = _interp(p1,   spice_hist, p_f_full)\n",
        "\n",
        "    # historical resampled on p_f_full and total\n",
        "    sa_h_pf    = _interp(p_h_v, sa_h_v,    p_f_full)\n",
        "    diff       = sa_f - sa_h_pf\n",
        "\n",
        "    # heave = total - spice\n",
        "    heave_pf   = diff - spice_pf\n",
        "    return spice_pf, heave_pf\n",
        "\n",
        "\n",
        "def _heave_spice_th(sa_h, th_h, p_h, sa_f, th_f, p_f):\n",
        "    \"\"\"\n",
        "    1D column -> (spice, heave) for TEMPERATURE on p_f levels (full grid).\n",
        "    \"\"\"\n",
        "    p_f_full = p_f.copy()\n",
        "\n",
        "    ok_h = np.isfinite(sa_h) & np.isfinite(th_h) & np.isfinite(p_h)\n",
        "    ok_f = np.isfinite(sa_f) & np.isfinite(th_f) & np.isfinite(p_f)\n",
        "    if ok_f.sum() < 2 or ok_h.sum() < 2: # Corrected condition order\n",
        "        n = p_f_full.size\n",
        "        return np.full(n, np.nan), np.full(n, np.nan)\n",
        "\n",
        "\n",
        "    sa_h_v, th_h_v, p_h_v = sa_h[ok_h], th_h[ok_h], p_h[ok_h]\n",
        "    sa_f_v, th_f_v, p_f_v = sa_f[ok_f], th_f[ok_f], p_f[ok_f]\n",
        "\n",
        "    t1_list, p1_list = [], []\n",
        "    for s0, t0, p0 in zip(sa_h_v, th_h_v, p_h_v):\n",
        "        s_ntp, t_ntp, p_ntp = ntp_bottle_to_cast(s0, t0, p0, sa_f_v, th_f_v, p_f_v)\n",
        "        t1_list.append(t_ntp); p1_list.append(p_ntp)\n",
        "    t1 = np.asarray(t1_list); p1 = np.asarray(p1_list)\n",
        "\n",
        "    spice_hist = t1 - th_h_v\n",
        "    spice_pf   = _interp(p1,   spice_hist, p_f_full)\n",
        "\n",
        "    th_h_pf    = _interp(p_h_v, th_h_v,    p_f_full)\n",
        "    diff       = th_f - th_h_pf\n",
        "    heave_pf   = diff - spice_pf\n",
        "    return spice_pf, heave_pf\n",
        "\n",
        "\n",
        "def ensure_same_lev(ds_h: xr.Dataset, ds_ssp: xr.Dataset, *, strict=False):\n",
        "    \"\"\"\n",
        "    Ensures ds_h and ds_ssp have the same 'lev' coordinate.\n",
        "    - strict=True: raises an error if they differ.\n",
        "    - strict=False: interpolates ds_h to ds_ssp's grid, if necessary.\n",
        "    Returns (ds_h2, ds_ssp2) with identical and sorted 'lev'.\n",
        "    \"\"\"\n",
        "    # sort for safety\n",
        "    ds_h   = ds_h.sortby('lev')\n",
        "    ds_ssp = ds_ssp.sortby('lev')\n",
        "\n",
        "    same_size = ds_h.sizes.get('lev', None) == ds_ssp.sizes.get('lev', None)\n",
        "    same_vals = same_size and np.allclose(ds_h['lev'].values, ds_ssp['lev'].values, equal_nan=False)\n",
        "\n",
        "    if same_vals:\n",
        "        return ds_h, ds_ssp  # already aligned\n",
        "\n",
        "    if strict:\n",
        "        raise ValueError(\n",
        "            \"ds_h and ds_ssp have different 'lev' coordinates. \"\n",
        "            f\"ds_h.lev.size={ds_h.sizes.get('lev')}, ds_ssp.lev.size={ds_ssp.sizes.get('lev')}\"\n",
        "        )\n",
        "\n",
        "    # align by interpolating the historical to the future grid\n",
        "    ds_h_interp = ds_h.interp(lev=ds_ssp['lev'])\n",
        "    return ds_h_interp, ds_ssp\n",
        "\n",
        "def get_nice_limits(vmax_abs):\n",
        "    \"\"\"\n",
        "    Calculates \"nice\" symmetric limits and contour levels\n",
        "    using Matplotlib's MaxNLocator.\n",
        "    \"\"\"\n",
        "    # Handles NaN or zero values\n",
        "    if vmax_abs == 0 or np.isnan(vmax_abs):\n",
        "        vmax_abs = 0.1 # Sets a small default to avoid errors\n",
        "\n",
        "    # Uses MaxNLocator to find \"nice\" tick locations (around 11)\n",
        "    # symmetric=True ensures ticks are symmetric around 0\n",
        "    # prune='both' removes ticks outside the data range\n",
        "    locator = mticker.MaxNLocator(nbins=10, symmetric=True, prune='both')\n",
        "\n",
        "    # Gets ticks for the range [-vmax, +vmax]\n",
        "    ticks = locator.tick_values(-vmax_abs, vmax_abs)\n",
        "\n",
        "    # The new \"nice\" limits are the minimum and maximum of these ticks\n",
        "    nice_vmin = ticks[0]\n",
        "    nice_vmax = ticks[-1]\n",
        "\n",
        "    # Contour line levels are the ticks themselves\n",
        "    line_levels = ticks.tolist()\n",
        "\n",
        "    # Fill levels are a finer linspace between these \"nice\" limits\n",
        "    fill_levels = np.linspace(nice_vmin, nice_vmax, 51).tolist()\n",
        "\n",
        "    return nice_vmin, nice_vmax, fill_levels, line_levels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c791914",
        "outputId": "5b3b21de-40b5-43dc-95e1-3148439628f5"
      },
      "source": [
        "# @title Setup: File paths, Models, and Scenarios\n",
        "nc_files = glob.glob('*.nc')\n",
        "scenarios = ['ssp370', 'ssp585']\n",
        "\n",
        "# Redefine models by extracting unique model names from nc_files\n",
        "models = list(set([f.split('.')[2] for f in nc_files if len(f.split('.')) >= 4]))\n",
        "print(f\"Identified models: {models}\")\n",
        "\n",
        "# Create a dictionary to organize the file paths by model and scenario\n",
        "file_dict = {}\n",
        "for f in nc_files:\n",
        "    parts = f.split('.')\n",
        "    if len(parts) >= 4:\n",
        "        model = parts[2]\n",
        "        scenario = parts[3]\n",
        "        if model not in file_dict:\n",
        "            file_dict[model] = {}\n",
        "        file_dict[model][scenario] = f\n",
        "    else:\n",
        "        print(f\"Skipping file with unexpected format: {f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified models: ['IPSL-CM6A-LR', 'CAMS-CSM1-0', 'GFDL-ESM4', 'MIROC6', 'CNRM-ESM2-1', 'CESM2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e129ab1b",
        "outputId": "67e28826-0454-485c-ccaa-7a0f8451f2d3"
      },
      "source": [
        "# @title Calculate and Save Raw Contour Limits\n",
        "# Create directory for preprocessed data\n",
        "output_data_dir = \"preprocessed_data\"\n",
        "os.makedirs(output_data_dir, exist_ok=True)\n",
        "\n",
        "raw_contour_limits = {}\n",
        "\n",
        "for model in models:\n",
        "    if 'historical' not in file_dict.get(model, {}):\n",
        "        print(f\"Skipping model {model} as historical data is missing.\")\n",
        "        continue\n",
        "\n",
        "    if model not in raw_contour_limits:\n",
        "        raw_contour_limits[model] = {}\n",
        "\n",
        "    for scenario in scenarios:\n",
        "        if scenario not in file_dict.get(model, {}):\n",
        "            print(f\"Skipping scenario {scenario} for model {model} as data is missing.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing model: {model}, scenario: {scenario}\")\n",
        "\n",
        "        # Open historical dataset\n",
        "        ds_h = xr.open_dataset(file_dict[model]['historical'])\n",
        "\n",
        "        # Open scenario dataset\n",
        "        ds_ssp = xr.open_dataset(file_dict[model][scenario])\n",
        "\n",
        "        # Ensure same 'lev' coordinate\n",
        "        try:\n",
        "            ds_h, ds_ssp = ensure_same_lev(ds_h, ds_ssp, strict=True)\n",
        "        except ValueError as e:\n",
        "            print(f\"Error ensuring same 'lev' for model {model}, scenario {scenario}: {e}\")\n",
        "            ds_h.close()\n",
        "            ds_ssp.close()\n",
        "            continue\n",
        "\n",
        "        # --- Heave/Spice Calculation ---\n",
        "        # Calculate heave and spice for salinity\n",
        "        spice_sa, heave_sa = xr.apply_ufunc(\n",
        "            _heave_spice_sa,\n",
        "            ds_h['sa'], ds_h['thetao'], ds_h['press'],\n",
        "            ds_ssp['sa'], ds_ssp['thetao'], ds_ssp['press'],\n",
        "            input_core_dims=[['lev'], ['lev'], ['lev'], ['lev'], ['lev'], ['lev']],\n",
        "            output_core_dims=[['lev'], ['lev']],\n",
        "            vectorize=True, dask='parallelized', output_dtypes=[float, float]\n",
        "        )\n",
        "\n",
        "        # Calculate heave and spice for temperature\n",
        "        spice_th, heave_th = xr.apply_ufunc(\n",
        "            _heave_spice_th,\n",
        "            ds_h['sa'], ds_h['thetao'], ds_h['press'],\n",
        "            ds_ssp['sa'], ds_ssp['thetao'], ds_ssp['press'],\n",
        "            input_core_dims=[['lev'], ['lev'], ['lev'], ['lev'], ['lev'], ['lev']],\n",
        "            output_core_dims=[['lev'], ['lev']],\n",
        "            vectorize=True, dask='parallelized', output_dtypes=[float, float]\n",
        "        )\n",
        "\n",
        "        # Create a new dataset to store the results\n",
        "        ds_out = xr.Dataset(\n",
        "            {\n",
        "                'spice_salinity'   : spice_sa.transpose('lev','lat','lon'),\n",
        "                'heave_salinity'   : heave_sa.transpose('lev','lat','lon'),\n",
        "                'spice_temperature': spice_th.transpose('lev','lat','lon'),\n",
        "                'heave_temperature': heave_th.transpose('lev','lat','lon'),\n",
        "            },\n",
        "            coords={'lev': ds_ssp.lev, 'lat': ds_ssp.lat, 'lon': ds_ssp.lon}\n",
        "        )\n",
        "\n",
        "        # Calculate zonal mean and select depths up to 2000m\n",
        "        heave_temp_zonal_mean = ds_out.heave_temperature.mean('lon').sel(lev=slice(0, 2000)).isel(lev=slice(None, None, -1))\n",
        "        spice_temp_zonal_mean = ds_out.spice_temperature.mean('lon').sel(lev=slice(0, 2000)).isel(lev=slice(None, None, -1))\n",
        "        heave_sal_zonal_mean = ds_out.heave_salinity.mean('lon').sel(lev=slice(0, 2000)).isel(lev=slice(None, None, -1))\n",
        "        spice_sal_zonal_mean = ds_out.spice_salinity.mean('lon').sel(lev=slice(0, 2000)).isel(lev=slice(None, None, -1))\n",
        "\n",
        "        # --- SAVE ZONAL MEAN DATA ---\n",
        "        ds_zonal = xr.Dataset({\n",
        "            'heave_temperature': heave_temp_zonal_mean,\n",
        "            'spice_temperature': spice_temp_zonal_mean,\n",
        "            'heave_salinity': heave_sal_zonal_mean,\n",
        "            'spice_salinity': spice_sal_zonal_mean,\n",
        "        })\n",
        "        output_path = os.path.join(output_data_dir, f\"zonal_mean_{model}_{scenario}.nc\")\n",
        "        ds_zonal.to_netcdf(output_path)\n",
        "        print(f\"Saved preprocessed data to {output_path}\")\n",
        "\n",
        "        # --- Calculate RAW limits ---\n",
        "        max_abs_heave_temp = np.nanmax(np.abs(heave_temp_zonal_mean))\n",
        "        max_abs_spice_temp = np.nanmax(np.abs(spice_temp_zonal_mean))\n",
        "        max_abs_heave_sal = np.nanmax(np.abs(heave_sal_zonal_mean))\n",
        "        max_abs_spice_sal = np.nanmax(np.abs(spice_sal_zonal_mean))\n",
        "\n",
        "        # --- Store RAW limits ---\n",
        "        raw_contour_limits[model][scenario] = {\n",
        "            'heave_temp': {'vmax': float(max_abs_heave_temp)},\n",
        "            'spice_temp': {'vmax': float(max_abs_spice_temp)},\n",
        "            'heave_sal': {'vmax': float(max_abs_heave_sal)},\n",
        "            'spice_sal': {'vmax': float(max_abs_spice_sal)},\n",
        "        }\n",
        "\n",
        "        # Close datasets\n",
        "        ds_h.close()\n",
        "        ds_ssp.close()\n",
        "        ds_out.close()\n",
        "        ds_zonal.close()\n",
        "\n",
        "# --- SAVE RAW LIMITS TO A JSON FILE ---\n",
        "limits_file = 'raw_contour_limits.json'\n",
        "with open(limits_file, 'w') as f:\n",
        "    json.dump(raw_contour_limits, f, indent=4)\n",
        "\n",
        "print(f\"Finished processing all data. Raw limits saved to {limits_file}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing model: IPSL-CM6A-LR, scenario: ssp370\n",
            "Saved preprocessed data to preprocessed_data/zonal_mean_IPSL-CM6A-LR_ssp370.nc\n",
            "Processing model: IPSL-CM6A-LR, scenario: ssp585\n",
            "Saved preprocessed data to preprocessed_data/zonal_mean_IPSL-CM6A-LR_ssp585.nc\n",
            "Processing model: CAMS-CSM1-0, scenario: ssp370\n",
            "Saved preprocessed data to preprocessed_data/zonal_mean_CAMS-CSM1-0_ssp370.nc\n",
            "Processing model: CAMS-CSM1-0, scenario: ssp585\n",
            "Saved preprocessed data to preprocessed_data/zonal_mean_CAMS-CSM1-0_ssp585.nc\n",
            "Processing model: GFDL-ESM4, scenario: ssp370\n",
            "Saved preprocessed data to preprocessed_data/zonal_mean_GFDL-ESM4_ssp370.nc\n",
            "Processing model: GFDL-ESM4, scenario: ssp585\n",
            "Saved preprocessed data to preprocessed_data/zonal_mean_GFDL-ESM4_ssp585.nc\n",
            "Processing model: MIROC6, scenario: ssp370\n",
            "Saved preprocessed data to preprocessed_data/zonal_mean_MIROC6_ssp370.nc\n",
            "Processing model: MIROC6, scenario: ssp585\n",
            "Saved preprocessed data to preprocessed_data/zonal_mean_MIROC6_ssp585.nc\n",
            "Processing model: CNRM-ESM2-1, scenario: ssp370\n",
            "Saved preprocessed data to preprocessed_data/zonal_mean_CNRM-ESM2-1_ssp370.nc\n",
            "Processing model: CNRM-ESM2-1, scenario: ssp585\n",
            "Saved preprocessed data to preprocessed_data/zonal_mean_CNRM-ESM2-1_ssp585.nc\n",
            "Processing model: CESM2, scenario: ssp370\n",
            "Saved preprocessed data to preprocessed_data/zonal_mean_CESM2_ssp370.nc\n",
            "Processing model: CESM2, scenario: ssp585\n",
            "Saved preprocessed data to preprocessed_data/zonal_mean_CESM2_ssp585.nc\n",
            "Finished processing all data. Raw limits saved to raw_contour_limits.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78b6d399",
        "outputId": "6abdd1d2-e7a3-4f7a-ff83-f81bd5101ad2"
      },
      "source": [
        "# @title Normalize Contour Limits\n",
        "print(\"Normalizing contour limits across all scenarios AND between heave/spice...\")\n",
        "\n",
        "try:\n",
        "    with open('raw_contour_limits.json', 'r') as f:\n",
        "        raw_contour_limits = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'raw_contour_limits.json' file not found.\")\n",
        "    print(\"Please run Step 1 script first.\")\n",
        "    # exit()\n",
        "\n",
        "final_plot_limits = {}\n",
        "\n",
        "for model in raw_contour_limits:\n",
        "    final_plot_limits[model] = {}\n",
        "\n",
        "    # 1. Find the absolute maximum value for Temp (Heave AND Spice)\n",
        "    #    and Sal (Heave AND Spice) ACROSS all scenarios\n",
        "    max_abs_temp = 0.0\n",
        "    max_abs_sal = 0.0\n",
        "\n",
        "    scenarios_for_model = list(raw_contour_limits[model].keys())\n",
        "    if not scenarios_for_model:\n",
        "        continue\n",
        "\n",
        "    for scenario in scenarios_for_model:\n",
        "        limits = raw_contour_limits[model][scenario]\n",
        "\n",
        "        # Compare the current max TEMP with heave_temp and spice_temp of this scenario\n",
        "        max_abs_temp = max(\n",
        "            max_abs_temp,\n",
        "            limits['heave_temp']['vmax'],\n",
        "            limits['spice_temp']['vmax']\n",
        "        )\n",
        "\n",
        "        # Compare the current max SAL with heave_sal and spice_sal of this scenario\n",
        "        max_abs_sal = max(\n",
        "            max_abs_sal,\n",
        "            limits['heave_sal']['vmax'],\n",
        "            limits['spice_sal']['vmax']\n",
        "        )\n",
        "\n",
        "    # 2. Get the \"nice\" limits based on these global maximums\n",
        "    vmin_t, vmax_t, fill_t, lines_t = get_nice_limits(max_abs_temp)\n",
        "    vmin_s, vmax_s, fill_s, lines_s = get_nice_limits(max_abs_sal)\n",
        "\n",
        "    # 3. Create the limit dictionaries.\n",
        "    #    Note that heave/spice for TEMP will use THE SAME '..._t' limits\n",
        "    global_nice_limits_temp = {\n",
        "        'vmin': vmin_t, 'vmax': vmax_t, 'fill_levels': fill_t, 'line_levels': lines_t\n",
        "    }\n",
        "    global_nice_limits_sal = {\n",
        "        'vmin': vmin_s, 'vmax': vmax_s, 'fill_levels': fill_s, 'line_levels': lines_s\n",
        "    }\n",
        "\n",
        "    # 4. Apply these limits to all scenarios for that model\n",
        "    for scenario in scenarios_for_model:\n",
        "         if scenario in raw_contour_limits[model]:\n",
        "            final_plot_limits[model][scenario] = {\n",
        "                'heave_temp': global_nice_limits_temp,\n",
        "                'spice_temp': global_nice_limits_temp, # SAME temp limit\n",
        "                'heave_sal': global_nice_limits_sal,\n",
        "                'spice_sal': global_nice_limits_sal,  # SAME sal limit\n",
        "            }\n",
        "\n",
        "# 5. Save the final and \"nice\" limits\n",
        "final_limits_file = 'final_plot_limits.json'\n",
        "with open(final_limits_file, 'w') as f:\n",
        "    json.dump(final_plot_limits, f, indent=4)\n",
        "\n",
        "print(f\"Normalization complete. Final limits saved to {final_limits_file}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizing contour limits across all scenarios AND between heave/spice...\n",
            "Normalization complete. Final limits saved to final_plot_limits.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "034968fa",
        "outputId": "107db4cf-d605-465f-a651-70555d759f72"
      },
      "source": [
        "# @title Calculate and Save Ensemble Mean Data\n",
        "ensemble_data = {}\n",
        "input_data_dir = \"preprocessed_data\"\n",
        "\n",
        "for scenario in scenarios:\n",
        "    print(f\"Calculating ensemble mean for scenario: {scenario}\")\n",
        "    scenario_datasets = []\n",
        "    valid_models_for_scenario = []\n",
        "\n",
        "    for model in models:\n",
        "        input_path = os.path.join(input_data_dir, f\"zonal_mean_{model}_{scenario}.nc\")\n",
        "        if os.path.exists(input_path):\n",
        "            try:\n",
        "                ds_zonal = xr.open_dataset(input_path)\n",
        "                scenario_datasets.append(ds_zonal)\n",
        "                valid_models_for_scenario.append(model)\n",
        "            except Exception as e:\n",
        "                print(f\"  Error opening dataset for model {model}, scenario {scenario}: {e}. Skipping.\")\n",
        "        # else:\n",
        "            # print(f\"  Missing preprocessed data for model {model}, scenario {scenario}. Skipping.\")\n",
        "\n",
        "    if not scenario_datasets:\n",
        "        print(f\"  No valid data found for scenario {scenario}. Cannot calculate ensemble mean.\")\n",
        "        continue\n",
        "\n",
        "    # Ensure all datasets in scenario_datasets have the same 'lev' before concatenating\n",
        "    # Find the union of all 'lev' coordinates across the datasets for this scenario\n",
        "    all_levs = sorted(list(set(np.concatenate([ds['lev'].values for ds in scenario_datasets]))))\n",
        "    target_lev = xr.DataArray(all_levs, dims=['lev'], coords={'lev': all_levs})\n",
        "\n",
        "    aligned_scenario_datasets = []\n",
        "    for ds in scenario_datasets:\n",
        "        try:\n",
        "            # Interpolate each dataset to the union of 'lev' coordinates\n",
        "            aligned_ds = ds.interp(lev=target_lev, method='linear', kwargs={\"fill_value\": \"extrapolate\"})\n",
        "            aligned_scenario_datasets.append(aligned_ds)\n",
        "        except Exception as e:\n",
        "            print(f\"  Error interpolating dataset for a model in scenario {scenario}: {e}. Skipping this dataset from ensemble.\")\n",
        "            ds.close() # Close the dataset if interpolation fails\n",
        "            continue\n",
        "        finally:\n",
        "             ds.close() # Close the original dataset after processing\n",
        "\n",
        "\n",
        "    if not aligned_scenario_datasets:\n",
        "        print(f\"  No datasets could be aligned for scenario {scenario}. Skipping ensemble mean calculation.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Concatenate and calculate the mean\n",
        "        ensemble_data[scenario] = xr.concat(aligned_scenario_datasets, dim='model').mean(dim='model')\n",
        "        print(f\"Finished calculating ensemble mean for scenario: {scenario}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error concatenating or calculating mean for scenario {scenario}: {e}. Skipping.\")\n",
        "        # Close any datasets in aligned_scenario_datasets that might still be open\n",
        "        for ds in aligned_scenario_datasets:\n",
        "             ds.close()\n",
        "        continue\n",
        "\n",
        "\n",
        "# Now, save the ensemble data\n",
        "output_data_dir = \"preprocessed_data\"\n",
        "os.makedirs(output_data_dir, exist_ok=True)\n",
        "\n",
        "for scenario, ds_ensemble in ensemble_data.items():\n",
        "    output_path = os.path.join(output_data_dir, f\"ensemble_zonal_mean_{scenario}.nc\")\n",
        "    ds_ensemble.to_netcdf(output_path)\n",
        "    print(f\"Saved ensemble data for scenario {scenario} to {output_path}\")\n",
        "    ds_ensemble.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating ensemble mean for scenario: ssp370\n",
            "Finished calculating ensemble mean for scenario: ssp370\n",
            "Calculating ensemble mean for scenario: ssp585\n",
            "Finished calculating ensemble mean for scenario: ssp585\n",
            "Saved ensemble data for scenario ssp370 to preprocessed_data/ensemble_zonal_mean_ssp370.nc\n",
            "Saved ensemble data for scenario ssp585 to preprocessed_data/ensemble_zonal_mean_ssp585.nc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f928ed9",
        "outputId": "ba5ee1d7-0d0e-4c21-8988-97a68c5ac998"
      },
      "source": [
        "# @title Generate Final Plots with Ensemble Mean and Stippling\n",
        "# --- Colormap Definition ---\n",
        "cmap_base = cm.get_cmap('RdBu_r', 256)\n",
        "white_index = 128\n",
        "colors = cmap_base(np.linspace(0, 1, 256))\n",
        "colors[white_index, :] = [1, 1, 1, 1]\n",
        "RdBu_white_centered = ListedColormap(colors)\n",
        "\n",
        "# --- Load FINALIZED limits ---\n",
        "try:\n",
        "    with open('final_plot_limits.json', 'r') as f:\n",
        "        final_plot_limits = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'final_plot_limits.json' file not found.\")\n",
        "    print(\"Please run Step 2 (normalization) script first.\")\n",
        "    # exit()\n",
        "\n",
        "input_data_dir = \"preprocessed_data\"\n",
        "NCOLS = 3 # Defines the number of model columns\n",
        "\n",
        "for scenario in scenarios:\n",
        "    print(f\"Generating plots for scenario: {scenario}\")\n",
        "\n",
        "    # 1. Count how many models have processed data for this scenario\n",
        "    models_to_plot = []\n",
        "    for model in models:\n",
        "        input_path = os.path.join(input_data_dir, f\"zonal_mean_{model}_{scenario}.nc\")\n",
        "        if os.path.exists(input_path):\n",
        "             models_to_plot.append(model)\n",
        "\n",
        "    n_models = len(models_to_plot)\n",
        "    if n_models == 0:\n",
        "        print(f\"No processed data found for scenario {scenario}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # 2. Calculate the figure layout\n",
        "    # Each model occupies 2 rows (Heave, Spice)\n",
        "    # Adds 2 extra rows for the ensemble (Heave, Spice)\n",
        "    n_model_rows = int(math.ceil(n_models / NCOLS))\n",
        "    n_fig_rows = n_model_rows * 2 + 2 # +2 for ensemble plots\n",
        "\n",
        "    # 3. Create TWO figures: one for Temp, one for Sal\n",
        "    # squeeze=False ensures 'axes' is always a 2D array\n",
        "    fig_temp, axes_temp = plt.subplots(\n",
        "        n_fig_rows, NCOLS,\n",
        "        figsize=(NCOLS * 8, n_fig_rows * 4), # (total_width, total_height)\n",
        "        layout='constrained',\n",
        "        squeeze=False\n",
        "    )\n",
        "\n",
        "    fig_sal, axes_sal = plt.subplots(\n",
        "        n_fig_rows, NCOLS,\n",
        "        figsize=(NCOLS * 8, n_fig_rows * 4),\n",
        "        layout='constrained',\n",
        "        squeeze=False\n",
        "    )\n",
        "\n",
        "    # --- 4. Loop for Plotting Individual Models ---\n",
        "    for i, model in enumerate(models_to_plot):\n",
        "        print(f\"  Plotting model: {model}\")\n",
        "\n",
        "        # Calculate position in the grid\n",
        "        col_idx = i % NCOLS\n",
        "        model_row_idx = i // NCOLS\n",
        "        heave_row_idx = model_row_idx * 2\n",
        "        spice_row_idx = model_row_idx * 2 + 1\n",
        "\n",
        "        # Select the correct axes\n",
        "        ax_ht = axes_temp[heave_row_idx, col_idx]\n",
        "        ax_st = axes_temp[spice_row_idx, col_idx]\n",
        "        ax_hs = axes_sal[heave_row_idx, col_idx]\n",
        "        ax_ss = axes_sal[spice_row_idx, col_idx]\n",
        "\n",
        "        # Load preprocessed data\n",
        "        input_path = os.path.join(input_data_dir, f\"zonal_mean_{model}_{scenario}.nc\")\n",
        "        ds_zonal = xr.open_dataset(input_path)\n",
        "\n",
        "        # Get limits (now identical for heave/spice)\n",
        "        limits = final_plot_limits[model][scenario]\n",
        "        limits_temp = limits['heave_temp'] # It's the same for heave_temp and spice_temp\n",
        "        limits_sal = limits['heave_sal']   # It's the same for heave_sal and spice_sal\n",
        "\n",
        "        # --- PLOT TEMPERATURE (Individual Model) ---\n",
        "\n",
        "        # Heave Temperature (Top Plot)\n",
        "        cf_ht = ax_ht.contourf(\n",
        "            ds_zonal['lat'], ds_zonal['lev'], ds_zonal['heave_temperature'],\n",
        "            levels=limits_temp['fill_levels'], cmap=RdBu_white_centered, extend='both',\n",
        "            vmin=limits_temp['vmin'], vmax=limits_temp['vmax']\n",
        "        )\n",
        "        cs_ht = ax_ht.contour(\n",
        "            ds_zonal['lat'], ds_zonal['lev'], ds_zonal['heave_temperature'],\n",
        "            levels=limits_temp['line_levels'], colors='k', linewidths=0.6\n",
        "        )\n",
        "        ax_ht.clabel(cs_ht, fmt='%0.1f', fontsize=7)\n",
        "        ax_ht.invert_yaxis()\n",
        "        ax_ht.set_title(f\"{model} - Heave (θ)\")\n",
        "        # Remove X-axis labels for all except the last row of models\n",
        "        if model_row_idx < n_model_rows - 1 or n_model_rows == 0: # If not the last model row or if there are no models\n",
        "             ax_ht.set_xticklabels([])\n",
        "        fig_temp.colorbar(cf_ht, ax=ax_ht, orientation='vertical', label='°C',\n",
        "                         ticks=limits_temp['line_levels'])\n",
        "\n",
        "        # Spice Temperature (Bottom Plot)\n",
        "        cf_st = ax_st.contourf(\n",
        "            ds_zonal['lat'], ds_zonal['lev'], ds_zonal['spice_temperature'],\n",
        "            levels=limits_temp['fill_levels'], cmap=RdBu_white_centered, extend='both',\n",
        "            vmin=limits_temp['vmin'], vmax=limits_temp['vmax']\n",
        "        )\n",
        "        cs_st = ax_st.contour(\n",
        "            ds_zonal['lat'], ds_zonal['lev'], ds_zonal['spice_temperature'],\n",
        "            levels=limits_temp['line_levels'], colors='k', linewidths=0.6\n",
        "        )\n",
        "        ax_st.clabel(cs_st, fmt='%0.1f', fontsize=7)\n",
        "        ax_st.invert_yaxis()\n",
        "        ax_st.set_title(f\"{model} - Spice (θ)\")\n",
        "        # Remove X-axis labels for all except the last row of models\n",
        "        if model_row_idx < n_model_rows - 1 or n_model_rows == 0: # If not the last model row or if there are no models\n",
        "             ax_st.set_xticklabels([])\n",
        "        else:\n",
        "             ax_st.set_xlabel(\"Latitude\") # Add label only to the last model row (if any)\n",
        "        fig_temp.colorbar(cf_st, ax=ax_st, orientation='vertical', label='°C',\n",
        "                         ticks=limits_temp['line_levels'])\n",
        "\n",
        "\n",
        "        # --- PLOT SALINITY (Individual Model) ---\n",
        "\n",
        "        # Heave Salinity (Top Plot)\n",
        "        cf_hs = ax_hs.contourf(\n",
        "            ds_zonal['lat'], ds_zonal['lev'], ds_zonal['heave_salinity'],\n",
        "            levels=limits_sal['fill_levels'], cmap=RdBu_white_centered, extend='both',\n",
        "            vmin=limits_sal['vmin'], vmax=limits_sal['vmax']\n",
        "        )\n",
        "        cs_hs = ax_hs.contour(\n",
        "            ds_zonal['lat'], ds_zonal['lev'], ds_zonal['heave_salinity'],\n",
        "            levels=limits_sal['line_levels'], colors='k', linewidths=0.6\n",
        "        )\n",
        "        ax_hs.clabel(cs_hs, fmt='%0.2f', fontsize=7)\n",
        "        ax_hs.invert_yaxis()\n",
        "        ax_hs.set_title(f\"{model} - Heave (Salinity)\")\n",
        "        # Remove X-axis labels for all except the last row of models\n",
        "        if model_row_idx < n_model_rows - 1 or n_model_rows == 0: # If not the last model row or if there are no models\n",
        "             ax_hs.set_xticklabels([])\n",
        "        fig_sal.colorbar(cf_hs, ax=ax_hs, orientation='vertical', label='g/kg',\n",
        "                         ticks=limits_sal['line_levels'])\n",
        "\n",
        "        # Spice Salinity (Bottom Plot)\n",
        "        cf_ss = ax_ss.contourf(\n",
        "            ds_zonal['lat'], ds_zonal['lev'], ds_zonal['spice_salinity'],\n",
        "            levels=limits_sal['fill_levels'], cmap=RdBu_white_centered, extend='both',\n",
        "            vmin=limits_sal['vmin'], vmax=limits_sal['vmax']\n",
        "        )\n",
        "        cs_ss = ax_ss.contour(\n",
        "            ds_zonal['lat'], ds_zonal['lev'], ds_zonal['spice_salinity'],\n",
        "            levels=limits_sal['line_levels'], colors='k', linewidths=0.6\n",
        "        )\n",
        "        ax_ss.clabel(cs_ss, fmt='%0.2f', fontsize=7)\n",
        "        ax_ss.invert_yaxis()\n",
        "        ax_ss.set_title(f\"{model} - Spice (Salinity)\")\n",
        "        # Remove X-axis labels for all except the last row of models\n",
        "        if model_row_idx < n_model_rows - 1 or n_model_rows == 0: # If not the last model row or if there are no models\n",
        "             ax_ss.set_xticklabels([])\n",
        "        else:\n",
        "             ax_ss.set_xlabel(\"Latitude\") # Add label only to the last model row (if any)\n",
        "        fig_sal.colorbar(cf_ss, ax=ax_ss, orientation='vertical', label='g/kg',\n",
        "                         ticks=limits_sal['line_levels'])\n",
        "\n",
        "        # Close the dataset\n",
        "        ds_zonal.close()\n",
        "\n",
        "        # --- Y-axis Title Management for Individual Models ---\n",
        "        # Only show \"Depth (m)\" in the left column (col_idx == 0)\n",
        "        if col_idx == 0:\n",
        "            ax_ht.set_ylabel(\"Depth (m)\")\n",
        "            ax_st.set_ylabel(\"Depth (m)\")\n",
        "            ax_hs.set_ylabel(\"Depth (m)\")\n",
        "            ax_ss.set_ylabel(\"Depth (m)\")\n",
        "        else:\n",
        "            ax_ht.set_yticklabels([])\n",
        "            ax_st.set_yticklabels([])\n",
        "            ax_hs.set_yticklabels([])\n",
        "            ax_ss.set_yticklabels([])\n",
        "\n",
        "    # --- 5. Plot the Ensemble Mean ---\n",
        "    ensemble_input_path = os.path.join(input_data_dir, f\"ensemble_zonal_mean_{scenario}.nc\")\n",
        "    if os.path.exists(ensemble_input_path):\n",
        "        print(f\"  Plotting Ensemble Mean for scenario: {scenario}\")\n",
        "        ds_ensemble_zonal = xr.open_dataset(ensemble_input_path)\n",
        "\n",
        "        # Get the limits from one of the models (since they are normalized across models)\n",
        "        first_model = models_to_plot[0] if models_to_plot else None\n",
        "        if first_model and first_model in final_plot_limits and scenario in final_plot_limits[first_model]:\n",
        "             limits_temp = final_plot_limits[first_model][scenario]['heave_temp']\n",
        "             limits_sal = final_plot_limits[first_model][scenario]['heave_sal']\n",
        "        else:\n",
        "             print(f\"  Could not retrieve normalized limits for ensemble plot for scenario {scenario}. Skipping ensemble plot.\")\n",
        "             ds_ensemble_zonal.close()\n",
        "             # Need to remove the allocated ensemble axes if plotting is skipped\n",
        "             for col_idx in range(NCOLS):\n",
        "                 if n_model_rows * 2 + 0 < n_fig_rows: # Check if first ensemble row exists\n",
        "                     fig_temp.delaxes(axes_temp[n_model_rows * 2 + 0, col_idx])\n",
        "                     fig_sal.delaxes(axes_sal[n_model_rows * 2 + 0, col_idx])\n",
        "                 if n_model_rows * 2 + 1 < n_fig_rows: # Check if second ensemble row exists\n",
        "                     fig_temp.delaxes(axes_temp[n_model_rows * 2 + 1, col_idx])\n",
        "                     fig_sal.delaxes(axes_sal[n_model_rows * 2 + 1, col_idx])\n",
        "             continue\n",
        "\n",
        "\n",
        "        # Calculate the starting row for the ensemble plots\n",
        "        ensemble_heave_row_idx = n_model_rows * 2\n",
        "        ensemble_spice_row_idx = n_model_rows * 2 + 1\n",
        "\n",
        "        # --- STATISTICAL SIGNIFICANCE CALCULATION (T-TEST) ---\n",
        "        # Need to load the individual model zonal mean data again to calculate std deviation across models\n",
        "        all_models_heave_temp = []\n",
        "        all_models_spice_temp = []\n",
        "        all_models_heave_sal = []\n",
        "        all_models_spice_sal = []\n",
        "\n",
        "        for model in models_to_plot:\n",
        "             input_path = os.path.join(input_data_dir, f\"zonal_mean_{model}_{scenario}.nc\")\n",
        "             if os.path.exists(input_path):\n",
        "                 try:\n",
        "                     ds_zonal_model = xr.open_dataset(input_path)\n",
        "                     # Ensure alignment before appending\n",
        "                     # Use the ensemble mean's 'lev' as the target\n",
        "                     target_lev = ds_ensemble_zonal['lev']\n",
        "                     all_models_heave_temp.append(ds_zonal_model['heave_temperature'].interp(lev=target_lev, method='linear', kwargs={\"fill_value\": \"extrapolate\"}))\n",
        "                     all_models_spice_temp.append(ds_zonal_model['spice_temperature'].interp(lev=target_lev, method='linear', kwargs={\"fill_value\": \"extrapolate\"}))\n",
        "                     all_models_heave_sal.append(ds_zonal_model['heave_salinity'].interp(lev=target_lev, method='linear', kwargs={\"fill_value\": \"extrapolate\"}))\n",
        "                     all_models_spice_sal.append(ds_zonal_model['spice_salinity'].interp(lev=target_lev, method='linear', kwargs={\"fill_value\": \"extrapolate\"}))\n",
        "                     ds_zonal_model.close()\n",
        "                 except Exception as e:\n",
        "                     print(f\"  Error loading or aligning individual model data for significance test for model {model}, scenario {scenario}: {e}. Skipping this model for stippling calculation.\")\n",
        "                     # Ensure ds_zonal_model is closed even on error\n",
        "                     if 'ds_zonal_model' in locals() and ds_zonal_model:\n",
        "                         ds_zonal_model.close()\n",
        "\n",
        "\n",
        "        n_models_for_stippling = len(all_models_heave_temp)\n",
        "\n",
        "        if n_models_for_stippling > 1:\n",
        "            # Concatenate aligned data along a new 'model' dimension\n",
        "            concatenated_heave_temp = xr.concat(all_models_heave_temp, dim='model')\n",
        "            concatenated_spice_temp = xr.concat(all_models_spice_temp, dim='model')\n",
        "            concatenated_heave_sal = xr.concat(all_models_heave_sal, dim='model')\n",
        "            concatenated_spice_sal = xr.concat(all_models_spice_sal, dim='model')\n",
        "\n",
        "            # Calculate standard deviation across the 'model' dimension\n",
        "            ensemble_std_heave_temp = concatenated_heave_temp.std(dim=\"model\")\n",
        "            ensemble_std_spice_temp = concatenated_spice_temp.std(dim=\"model\")\n",
        "            ensemble_std_heave_sal = concatenated_heave_sal.std(dim=\"model\")\n",
        "            ensemble_std_spice_sal = concatenated_spice_sal.std(dim=\"model\")\n",
        "\n",
        "            # Calculate the t-statistic: t = (mean - 0) / (std / sqrt(n))\n",
        "            # Mean is the ensemble mean loaded from ds_ensemble_zonal\n",
        "            with np.errstate(divide='ignore', invalid='ignore'):\n",
        "                t_stat_heave_temp = (ds_ensemble_zonal['heave_temperature'] / (ensemble_std_heave_temp / np.sqrt(n_models_for_stippling))).fillna(0)\n",
        "                t_stat_spice_temp = (ds_ensemble_zonal['spice_temperature'] / (ensemble_std_spice_temp / np.sqrt(n_models_for_stippling))).fillna(0)\n",
        "                t_stat_heave_sal = (ds_ensemble_zonal['heave_salinity'] / (ensemble_std_heave_sal / np.sqrt(n_models_for_stippling))).fillna(0)\n",
        "                t_stat_spice_sal = (ds_ensemble_zonal['spice_salinity'] / (ensemble_std_spice_sal / np.sqrt(n_models_for_stippling))).fillna(0)\n",
        "\n",
        "            # Degrees of freedom = n - 1\n",
        "            df = n_models_for_stippling - 1\n",
        "\n",
        "            # Calculate p-values (using numpy arrays and scipy.stats)\n",
        "            p_values_heave_temp_np = stats.t.sf(np.abs(t_stat_heave_temp.values), df=df) * 2\n",
        "            p_values_spice_temp_np = stats.t.sf(np.abs(t_stat_spice_temp.values), df=df) * 2\n",
        "            p_values_heave_sal_np = stats.t.sf(np.abs(t_stat_heave_sal.values), df=df) * 2\n",
        "            p_values_spice_sal_np = stats.t.sf(np.abs(t_stat_spice_sal.values), df=df) * 2\n",
        "\n",
        "            # Convert p_values back to xarray.DataArray with coordinates\n",
        "            p_values_heave_temp_da = xr.DataArray(p_values_heave_temp_np, coords=t_stat_heave_temp.coords, dims=t_stat_heave_temp.dims)\n",
        "            p_values_spice_temp_da = xr.DataArray(p_values_spice_temp_np, coords=t_stat_spice_temp.coords, dims=t_stat_spice_temp.dims)\n",
        "            p_values_heave_sal_da = xr.DataArray(p_values_heave_sal_np, coords=t_stat_heave_sal.coords, dims=t_stat_heave_sal.dims)\n",
        "            p_values_spice_sal_da = xr.DataArray(p_values_spice_sal_np, coords=t_stat_spice_sal.coords, dims=t_stat_spice_sal.dims)\n",
        "\n",
        "            # Create the significance mask (where p < 0.05, put 1, otherwise NaN)\n",
        "            significance_mask_heave_temp = xr.where(p_values_heave_temp_da < 0.05, 1, np.nan)\n",
        "            significance_mask_spice_temp = xr.where(p_values_spice_temp_da < 0.05, 1, np.nan)\n",
        "            significance_mask_heave_sal = xr.where(p_values_heave_sal_da < 0.05, 1, np.nan)\n",
        "            significance_mask_spice_sal = xr.where(p_values_spice_sal_da < 0.05, 1, np.nan)\n",
        "\n",
        "        else:\n",
        "            significance_mask_heave_temp = None\n",
        "            significance_mask_spice_temp = None\n",
        "            significance_mask_heave_sal = None\n",
        "            significance_mask_spice_sal = None\n",
        "            print(f\"  Not enough models ({n_models_for_stippling}) for significance test for scenario {scenario}. Skipping stippling.\")\n",
        "\n",
        "\n",
        "        # --- PLOT TEMPERATURE (Ensemble Mean) ---\n",
        "\n",
        "        # Heave Temperature (Ensemble)\n",
        "        ax_ht_ens = axes_temp[ensemble_heave_row_idx, 0] # Always in the first column\n",
        "        cf_ht_ens = ax_ht_ens.contourf(\n",
        "            ds_ensemble_zonal['lat'], ds_ensemble_zonal['lev'], ds_ensemble_zonal['heave_temperature'],\n",
        "            levels=limits_temp['fill_levels'], cmap=RdBu_white_centered, extend='both',\n",
        "            vmin=limits_temp['vmin'], vmax=limits_temp['vmax']\n",
        "        )\n",
        "        cs_ht_ens = ax_ht_ens.contour(\n",
        "            ds_ensemble_zonal['lat'], ds_ensemble_zonal['lev'], ds_ensemble_zonal['heave_temperature'],\n",
        "            levels=limits_temp['line_levels'], colors='k', linewidths=0.6\n",
        "        )\n",
        "        ax_ht_ens.clabel(cs_ht_ens, fmt='%0.1f', fontsize=7)\n",
        "        ax_ht_ens.invert_yaxis()\n",
        "        ax_ht_ens.set_title(f\"Ensemble Mean - Heave (θ)\")\n",
        "        ax_ht_ens.set_xlabel(\"Latitude\") # Add xlabel to the bottom row\n",
        "        ax_ht_ens.set_ylabel(\"Depth (m)\") # Add ylabel to the leftmost column\n",
        "        fig_temp.colorbar(cf_ht_ens, ax=ax_ht_ens, orientation='vertical', label='°C',\n",
        "                         ticks=limits_temp['line_levels'])\n",
        "\n",
        "        # Add stippling for Heave Temperature if available\n",
        "        if significance_mask_heave_temp is not None:\n",
        "            ax_ht_ens.contourf(\n",
        "                significance_mask_heave_temp['lat'], significance_mask_heave_temp['lev'], significance_mask_heave_temp,\n",
        "                levels=[0.5, 1.5], # Plot where mask is 1\n",
        "                hatches=['...'], colors='none' # Use dots for stippling\n",
        "            )\n",
        "\n",
        "\n",
        "        # Spice Temperature (Ensemble)\n",
        "        ax_st_ens = axes_temp[ensemble_spice_row_idx, 0] # Always in the first column\n",
        "        cf_st_ens = ax_st_ens.contourf(\n",
        "            ds_ensemble_zonal['lat'], ds_ensemble_zonal['lev'], ds_ensemble_zonal['spice_temperature'],\n",
        "            levels=limits_temp['fill_levels'], cmap=RdBu_white_centered, extend='both',\n",
        "            vmin=limits_temp['vmin'], vmax=limits_temp['vmax']\n",
        "        )\n",
        "        cs_st_ens = ax_st_ens.contour(\n",
        "            ds_ensemble_zonal['lat'], ds_ensemble_zonal['lev'], ds_ensemble_zonal['spice_temperature'],\n",
        "            levels=limits_temp['line_levels'], colors='k', linewidths=0.6\n",
        "        )\n",
        "        ax_st_ens.clabel(cs_st_ens, fmt='%0.1f', fontsize=7)\n",
        "        ax_st_ens.invert_yaxis()\n",
        "        ax_st_ens.set_title(f\"Ensemble Mean - Spice (θ)\")\n",
        "        ax_st_ens.set_xlabel(\"Latitude\") # Add xlabel to the bottom row\n",
        "        ax_st_ens.set_ylabel(\"Depth (m)\") # Add ylabel to the leftmost column\n",
        "        fig_temp.colorbar(cf_st_ens, ax=ax_st_ens, orientation='vertical', label='°C',\n",
        "                         ticks=limits_temp['line_levels'])\n",
        "\n",
        "        # Add stippling for Spice Temperature if available\n",
        "        if significance_mask_spice_temp is not None:\n",
        "            ax_st_ens.contourf(\n",
        "                significance_mask_spice_temp['lat'], significance_mask_spice_temp['lev'], significance_mask_spice_temp,\n",
        "                levels=[0.5, 1.5], # Plot where mask is 1\n",
        "                hatches=['...'], colors='none' # Use dots for stippling\n",
        "            )\n",
        "\n",
        "\n",
        "        # --- PLOT SALINITY (Ensemble Mean) ---\n",
        "\n",
        "        # Heave Salinity (Ensemble)\n",
        "        ax_hs_ens = axes_sal[ensemble_heave_row_idx, 0] # Always in the first column\n",
        "        cf_hs_ens = ax_hs_ens.contourf(\n",
        "            ds_ensemble_zonal['lat'], ds_ensemble_zonal['lev'], ds_ensemble_zonal['heave_salinity'],\n",
        "            levels=limits_sal['fill_levels'], cmap=RdBu_white_centered, extend='both',\n",
        "            vmin=limits_sal['vmin'], vmax=limits_sal['vmax']\n",
        "        )\n",
        "        cs_hs_ens = ax_hs_ens.contour(\n",
        "            ds_ensemble_zonal['lat'], ds_ensemble_zonal['lev'], ds_ensemble_zonal['heave_salinity'],\n",
        "            levels=limits_sal['line_levels'], colors='k', linewidths=0.6\n",
        "        )\n",
        "        ax_hs_ens.clabel(cs_hs_ens, fmt='%0.2f', fontsize=7)\n",
        "        ax_hs_ens.invert_yaxis()\n",
        "        ax_hs_ens.set_title(f\"Ensemble Mean - Heave (Salinity)\")\n",
        "        ax_hs_ens.set_xlabel(\"Latitude\") # Add xlabel to the bottom row\n",
        "        ax_hs_ens.set_ylabel(\"Depth (m)\") # Add ylabel to the leftmost column\n",
        "        fig_sal.colorbar(cf_hs_ens, ax=ax_hs_ens, orientation='vertical', label='g/kg',\n",
        "                         ticks=limits_sal['line_levels'])\n",
        "\n",
        "        # Add stippling for Heave Salinity if available\n",
        "        if significance_mask_heave_sal is not None:\n",
        "            ax_hs_ens.contourf(\n",
        "                significance_mask_heave_sal['lat'], significance_mask_heave_sal['lev'], significance_mask_heave_sal,\n",
        "                levels=[0.5, 1.5], # Plot where mask is 1\n",
        "                hatches=['...'], colors='none' # Use dots for stippling\n",
        "            )\n",
        "\n",
        "\n",
        "        # Spice Salinity (Ensemble)\n",
        "        ax_ss_ens = axes_sal[ensemble_spice_row_idx, 0] # Always in the first column\n",
        "        cf_ss_ens = ax_ss_ens.contourf(\n",
        "            ds_ensemble_zonal['lat'], ds_ensemble_zonal['lev'], ds_ensemble_zonal['spice_salinity'],\n",
        "            levels=limits_sal['fill_levels'], cmap=RdBu_white_centered, extend='both',\n",
        "            vmin=limits_sal['vmin'], vmax=limits_sal['vmax']\n",
        "        )\n",
        "        cs_ss_ens = ax_ss_ens.contour(\n",
        "            ds_ensemble_zonal['lat'], ds_ensemble_zonal['lev'], ds_ensemble_zonal['spice_salinity'],\n",
        "            levels=limits_sal['line_levels'], colors='k', linewidths=0.6\n",
        "        )\n",
        "        ax_ss_ens.clabel(cs_ss_ens, fmt='%0.2f', fontsize=7)\n",
        "        ax_ss_ens.invert_yaxis()\n",
        "        ax_ss_ens.set_title(f\"Ensemble Mean - Spice (Salinity)\")\n",
        "        ax_ss_ens.set_xlabel(\"Latitude\") # Add xlabel to the bottom row\n",
        "        ax_ss_ens.set_ylabel(\"Depth (m)\") # Add ylabel to the leftmost column\n",
        "        fig_sal.colorbar(cf_ss_ens, ax=ax_ss_ens, orientation='vertical', label='g/kg',\n",
        "                         ticks=limits_sal['line_levels'])\n",
        "\n",
        "        # Add stippling for Spice Salinity if available\n",
        "        if significance_mask_spice_sal is not None:\n",
        "            ax_ss_ens.contourf(\n",
        "                significance_mask_spice_sal['lat'], significance_mask_spice_sal['lev'], significance_mask_spice_sal,\n",
        "                levels=[0.5, 1.5], # Plot where mask is 1\n",
        "                hatches=['...'], colors='none' # Use dots for stippling\n",
        "            )\n",
        "\n",
        "        # Close the ensemble dataset\n",
        "        ds_ensemble_zonal.close()\n",
        "\n",
        "        # --- Clean up unused ensemble axes ---\n",
        "        # The ensemble only uses the first column. Remove remaining axes in the ensemble rows.\n",
        "        for col_idx in range(1, NCOLS):\n",
        "             if ensemble_heave_row_idx < n_fig_rows:\n",
        "                 fig_temp.delaxes(axes_temp[ensemble_heave_row_idx, col_idx])\n",
        "                 fig_sal.delaxes(axes_sal[ensemble_heave_row_idx, col_idx])\n",
        "             if ensemble_spice_row_idx < n_fig_rows:\n",
        "                 fig_temp.delaxes(axes_temp[ensemble_spice_row_idx, col_idx])\n",
        "                 fig_sal.delaxes(axes_sal[ensemble_spice_row_idx, col_idx])\n",
        "\n",
        "\n",
        "    # --- 6. Clean up remaining empty axes (if any) ---\n",
        "    # This part handles any axes in the last model row that weren't filled,\n",
        "    # and is still needed even with the dedicated ensemble row.\n",
        "    total_model_plots = n_models * 2 # 2 plots per model (Heave, Spice)\n",
        "    for i in range(total_model_plots, (n_model_rows * 2) * NCOLS):\n",
        "        row_idx = i // NCOLS\n",
        "        col_idx = i % NCOLS\n",
        "        if row_idx < n_fig_rows: # Ensure we don't go out of bounds\n",
        "             fig_temp.delaxes(axes_temp[row_idx, col_idx])\n",
        "             fig_sal.delaxes(axes_sal[row_idx, col_idx])\n",
        "\n",
        "\n",
        "    # --- 7. Save the Figures ---\n",
        "    fig_temp.savefig(f'{scenario}_temperature_plots_with_ensemble_stippling.jpeg', dpi=600, bbox_inches='tight')\n",
        "    fig_sal.savefig(f'{scenario}_salinity_plots_with_ensemble_stippling.jpeg', dpi=600, bbox_inches='tight')\n",
        "\n",
        "    plt.close(fig_temp)\n",
        "    plt.close(fig_sal)\n",
        "    print(f\"Finished generating plots for scenario: {scenario}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3550283467.py:3: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "  cmap_base = cm.get_cmap('RdBu_r', 256)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating plots for scenario: ssp370\n",
            "  Plotting model: IPSL-CM6A-LR\n",
            "  Plotting model: CAMS-CSM1-0\n",
            "  Plotting model: GFDL-ESM4\n",
            "  Plotting model: MIROC6\n",
            "  Plotting model: CNRM-ESM2-1\n",
            "  Plotting model: CESM2\n",
            "  Plotting Ensemble Mean for scenario: ssp370\n",
            "Finished generating plots for scenario: ssp370\n",
            "Generating plots for scenario: ssp585\n",
            "  Plotting model: IPSL-CM6A-LR\n",
            "  Plotting model: CAMS-CSM1-0\n",
            "  Plotting model: GFDL-ESM4\n",
            "  Plotting model: MIROC6\n",
            "  Plotting model: CNRM-ESM2-1\n",
            "  Plotting model: CESM2\n",
            "  Plotting Ensemble Mean for scenario: ssp585\n",
            "Finished generating plots for scenario: ssp585\n"
          ]
        }
      ]
    }
  ]
}